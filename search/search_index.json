{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLPerf Automotive Benchmarks","text":""},{"location":"#overview","title":"Overview","text":"<p>The currently valid MLPerf Automotive Benchmarks as of MLPerf inference v5.0 round are listed below, categorized by tasks. Under each model you can find its details like the dataset used, reference accuracy, server latency constraints etc.</p>"},{"location":"#2d-object-detection","title":"2D Object Detection","text":""},{"location":"#ssd-resnet50","title":"SSD-ResNet50","text":"<ul> <li>Dataset: Cognata<ul> <li>Dataset Size: TBD</li> <li>QSL Size: 128 (default, minimum needed)</li> </ul> </li> <li>Number of Parameters: TBD</li> <li>FLOPs: TBD</li> <li>Reference Model Accuracy: 0.7179 mAP</li> <li>Latency Target: 99.9%</li> <li>Accuracy Constraint: 99.9%</li> <li>Framework Support: ONNX, PyTorch</li> <li>Submission Category: Edge</li> </ul>"},{"location":"#camera-based-3d-object-detection","title":"Camera-Based 3D Object Detection","text":""},{"location":"#bevformer-tiny","title":"BEVFormer (tiny)","text":"<ul> <li>Dataset: NuScenes<ul> <li>Dataset Size: TBD</li> <li>QSL Size: 1024 (default), 512 (minimum needed)</li> </ul> </li> <li>Number of Parameters: TBD</li> <li>FLOPs: TBD</li> <li>Reference Model Accuracy: 0.2683556 mAP / 0.37884288 NDS</li> <li>Latency Target: 99.9%</li> <li>Accuracy Constraint: 99%</li> <li>Framework Support: ONNX, PyTorch</li> <li>Submission Category: Edge</li> </ul>"},{"location":"#semantic-segmentation","title":"Semantic Segmentation","text":""},{"location":"#deeplabv3","title":"DeepLabv3+","text":"<ul> <li>Dataset: Cognata<ul> <li>Dataset Size: TBD</li> <li>QSL Size: 128 (default, minimum needed)</li> </ul> </li> <li>Number of Parameters: TBD</li> <li>FLOPs: TBD</li> <li>Reference Model Accuracy: 0.924355 mIOU</li> <li>Resolution: 8MP</li> <li>Latency Target: 99.9%</li> <li>Accuracy Constraint: 99.9%</li> <li>Framework Support: ONNX (8MP &amp; Dynamic), PyTorch</li> <li>Submission Category: Edge</li> </ul>"},{"location":"#submission-categories","title":"Submission Categories","text":"<ul> <li>Edge Category: All benchmarks are applicable to the edge category for the automotive inference v0.5</li> </ul>"},{"location":"#high-accuracy-variants","title":"High Accuracy Variants","text":"<ul> <li>Benchmarks: All the benchmarks for submission round v0.5 have high accuracy variant.</li> <li>Requirement: Must achieve at least 99.9% of the reference model accuracy.</li> </ul>"},{"location":"index_gh/","title":"MLPerf\u2122 Automotive Benchmark Suite","text":"<p>MLPerf Automotive is a benchmark suite for measuring how fast automotive systems can run models in a variety of deployment scenarios. </p>"},{"location":"index_gh/#mlperf-automotive-v05","title":"MLPerf Automotive v0.5","text":"<p>Use the r0.5 branch (<code>git checkout v0.5abtf</code>) if you want to reproduce v0.5 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset ssd-resnet50 v0.5/2D object detection pytorch, onnx Cognata bevformer-tiny v0.5/camera-based 3D object detection pytorch, onnx NuScenes DeepLabV3+ v0.5/semantic segmentation pytorch, onnx Cognata"},{"location":"benchmarks/2d-object-detection/get-ssd-data/","title":"2D Object Detection using SSD-ResNet50","text":""},{"location":"benchmarks/2d-object-detection/get-ssd-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Preprocessed DatasetRaw Dataset <p>SSD validation run uses the Cognata dataset. The preprocessed data is located in mlc_cognata_dataset/preprocess_2d folder.</p> <p>For preprocessing the dataset yourself, you can download the raw dataset.</p>"},{"location":"benchmarks/2d-object-detection/get-ssd-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,cognata,_mlc,_2d_obj_det,_validation --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/2d-object-detection/get-ssd-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,cognata,_mlc,_2d_obj_det,_calibration --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/2d-object-detection/get-ssd-data/#get-raw-dataset","title":"Get Raw Dataset","text":"<pre><code>mlcr get,raw,dataset,cognata,_mlc,_rclone --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/2d-object-detection/get-ssd-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf SSD Model</p> ONNXPyTorch"},{"location":"benchmarks/2d-object-detection/get-ssd-data/#onnx","title":"ONNX","text":"<pre><code>mlcr get,ml-model,ssd,resnet50,_mlc,_rclone,_onnx --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/2d-object-detection/get-ssd-data/#pytorch","title":"PyTorch","text":"<pre><code>mlcr get,ml-model,ssd,resnet50,_mlc,_rclone,_pytorch --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/","title":"2D Object Detection using SSD ResNet50","text":"MLCommons-Python"},{"location":"benchmarks/2d-object-detection/ssd/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>SSD</p> edge"},{"location":"benchmarks/2d-object-detection/ssd/#edge-category","title":"Edge category","text":"<p>In the edge category, ssd has SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/2d-object-detection/ssd/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPU"},{"location":"benchmarks/2d-object-detection/ssd/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 750GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/2d-object-detection/ssd/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/2d-object-detection/ssd/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#singlestream","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/2d-object-detection/ssd/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#singlestream_1","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"benchmarks/2d-object-detection/ssd/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 750GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/2d-object-detection/ssd/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/2d-object-detection/ssd/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#singlestream_2","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/2d-object-detection/ssd/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#singlestream_3","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 750GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/2d-object-detection/ssd/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/2d-object-detection/ssd/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#singlestream_4","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/2d-object-detection/ssd/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for ssd you can follow this README.</li> </ul>"},{"location":"benchmarks/2d-object-detection/ssd/#singlestream_5","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=ssd \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/camera-3d-detection/bevformer/","title":"3D Object Detection using BEVFormer","text":"MLCommons-Python"},{"location":"benchmarks/camera-3d-detection/bevformer/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BEVFORMER</p> edge"},{"location":"benchmarks/camera-3d-detection/bevformer/#edge-category","title":"Edge category","text":"<p>In the edge category, bevformer has SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/camera-3d-detection/bevformer/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPU"},{"location":"benchmarks/camera-3d-detection/bevformer/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 350GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/camera-3d-detection/bevformer/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/camera-3d-detection/bevformer/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/camera-3d-detection/bevformer/#singlestream","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/camera-3d-detection/bevformer/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/camera-3d-detection/bevformer/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/camera-3d-detection/bevformer/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/camera-3d-detection/bevformer/#singlestream_1","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/camera-3d-detection/bevformer/#pytorch-framework","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/camera-3d-detection/bevformer/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 350GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/camera-3d-detection/bevformer/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/camera-3d-detection/bevformer/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/camera-3d-detection/bevformer/#singlestream_2","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/camera-3d-detection/bevformer/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/camera-3d-detection/bevformer/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/camera-3d-detection/bevformer/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for bevformer you can follow this README.</li> </ul>"},{"location":"benchmarks/camera-3d-detection/bevformer/#singlestream_3","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=bevformer \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/","title":"3D Object Detection using BEVFormer (tiny)","text":""},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Preprocessed DatasetRaw Dataset <p>BEVFormer validation run uses the NuScenes dataset. The preprocessed data is located in nuscenes_data/preprocessed/val_3d.tar.gz.</p> <p>For preprocessing the dataset yourself, you can download the raw dataset.</p>"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,nuscenes,_mlc,_validation --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,nuscenes,_mlc,_calibration --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#get-raw-dataset","title":"Get Raw Dataset","text":"<pre><code>mlcr get,dataset,nuscenes,_mlc,_rclone --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf BEVFormer Model</p> ONNXPyTorch"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#onnx","title":"ONNX","text":"<pre><code>mlcr get,ml-model,bevformer,_mlc,_rclone,_onnx --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/camera-3d-detection/get-bevformer-data/#pytorch","title":"PyTorch","text":"<pre><code>mlcr get,ml-model,bevformer,_mlc,_rclone,_pytorch --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/","title":"Semantic Segmentation using DeepLabv3+","text":"MLCommons-Python"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>DEEPLABV3PLUS</p> edge"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#edge-category","title":"Edge category","text":"<p>In the edge category, deeplabv3plus has SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUGPU"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 1.2TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_1","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#gpu-device","title":"GPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 1.2TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=gpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_2","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=gpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=gpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_3","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=gpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#pytorch-framework","title":"Pytorch framework","text":"CPUGPU"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 1.2TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_4","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_5","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#gpu-device_1","title":"GPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 20GB</p> </li> <li> <p>Disk Space: 1.2TB</p> </li> </ul> DockerNative"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=gpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_6","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=gpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>mlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>mlcr run-abtf-inference,reference,_find-performance,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=gpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for deeplabv3plus you can follow this README.</li> </ul>"},{"location":"benchmarks/semantic-segmentation/deeplabv3plus/#singlestream_7","title":"SingleStream","text":"<pre><code>mlcr run-abtf-inference,reference,_full,_v0.5 \\\n   --model=deeplabv3plus \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=gpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/","title":"Semantic Segmentation using DeepLabv3+","text":""},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Preprocessed DatasetRaw Dataset <p>DeepLabv3+ validation run uses the Cognata dataset. The preprocessed data is located under mlc_cognata_dataset/preprocessed_seg.</p> <p>For preprocessing the dataset yourself, you can download the raw dataset.</p>"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,cognata,_mlc,_segmentation,_validation --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>mlcr get,preprocessed,dataset,cognata,_mlc,_segmentation,_calibration --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#get-raw-dataset","title":"Get Raw Dataset","text":"<pre><code>mlcr get,raw,dataset,cognata,_mlc,_rclone --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf DeepLabv3+ Model</p> ONNXPyTorch"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#onnx","title":"ONNX","text":"<p>Two variants are available: <pre><code># 8MP optimized version\nmlcr get,ml-model,deeplabv3-plus,_mlc,_rclone,_onnx --outdirname=&lt;path_to_download&gt;\n\n# Dynamic resolution version\nmlcr get,ml-model,deeplabv3-plus,_mlc,_rclone,_onnx,_dynamic --outdirname=&lt;path_to_download&gt;\n</code></pre></p>"},{"location":"benchmarks/semantic-segmentation/get-deeplabv3plus-data/#pytorch","title":"PyTorch","text":"<pre><code>mlcr get,ml-model,deeplabv3-plus,_mlc,_rclone,_pytorch --outdirname=&lt;path_to_download&gt;\n</code></pre>"},{"location":"changelog/","title":"What's New &amp; What's Coming \ud83d\ude80","text":"<p>Info</p> <p>Inference Automotive v0.5 Submission is approaching! The submission deadline is TBD.  </p> <p>Tip</p> <p>Starting January 2025, MLPerf Inference automations are powered by MLCFlow\u2014a newly developed Python package that replaces the previously used CMind package. This transition enhances automation, streamlines workflows, and makes MLPerf scripts more independent and simpler.  </p>"},{"location":"changelog/changelog/","title":"Release Notes","text":"<p>\ud83d\ude80 mlc-scripts 1.0.0 was released on February 10, 2025, introducing full support for MLPerf Inference v5.0 using the first stable release of MLCFlow 1.0.1.  </p> <p>\ud83d\udd39 All previous CM scripts used in MLPerf have been successfully ported to the MLC interface, ensuring seamless integration. Additionally, all GitHub Actions are now passing, confirming a stable and reliable implementation.  </p>"},{"location":"changelog/changelog/#key-updates-in-mlcflow","title":"Key Updates in MLCFlow","text":"<p>\u2705 Simplified Interface - A redesigned approach using Actions and Targets, making the CLI more intuitive for users.  </p> <p>\u2705 Unified Automation Model - Consolidated into a single automation entity: Script, which is seamlessly extended by Cache, Docker, and Tests.  </p> <p>\u2705 Improved Docker Integration - A cleaner, more efficient Docker extension, streamlining containerized execution.  </p> <p>\u2705 Enhanced Script Management - Tighter integration between the interface and script automation, making script creation and management easier than ever.  </p>"},{"location":"demos/","title":"Demos","text":""},{"location":"install/","title":"Installation","text":"<p>We use MLCommons MLC Automation framework to run MLPerf inference benchmarks.</p> <p>MLC needs <code>git</code>, <code>python3-pip</code> and <code>python3-venv</code> installed on your system. Once the dependencies are installed, do the following</p>"},{"location":"install/#activate-a-virtual-env-for-mlcflow","title":"Activate a Virtual ENV for MLCFlow","text":"<p>This step is not mandatory as MLC can use separate virtual environment for MLPerf inference. But the latest <code>pip</code> install requires this or else will need the <code>--break-system-packages</code> flag while installing <code>mlc-scripts</code>.</p> <pre><code>python3 -m venv mlc\nsource mlc/bin/activate\n</code></pre>"},{"location":"install/#install-mlc-and-pulls-any-needed-repositories","title":"Install MLC and pulls any needed repositories","text":"Use the default fork of MLC-Scripts repositoryUse custom fork/branch of the MLC-Scripts repository <pre><code> pip install mlc-scripts\n</code></pre> <p><pre><code> pip install mlcflow &amp;&amp; mlc pull repo --url=mlcommons@cm4mlops --branch=mlperf-inference\n</code></pre> Here, <code>repo</code> is in the format <code>githubUsername@githubRepo</code>.</p> <p>Now, you are ready to use the <code>mlcr</code> commands to run MLPerf inference as given in the benchmarks page</p>"},{"location":"power/","title":"Power Measurement","text":"<p>Originally Prepared by the MLCommons taskforce on automation and reproducibility and OctoML.</p>"},{"location":"power/#requirements","title":"Requirements","text":"<ol> <li> <p>Power analyzer (anyone certified by SPEC PTDaemon). Yokogawa is the one that most submitters have submitted with and a new single-channel model like 310E can cost around 3000$. </p> </li> <li> <p>SPEC PTDaemon (can be downloaded from here after signing the EULA which can be requested by sending an email to <code>support@mlcommons.org</code>). Once you have GitHub access to the MLCommons power repository then the MLC workflow will automatically download and configure the SPEC PTDaemon tool.</p> </li> <li> <p>Access to the MLCommons power-dev repository which has the <code>server.py</code> to be run on the director node and <code>client.py</code> to be run on the SUT node. This repository being public will be automatically pulled by the MLC workflow.</p> </li> </ol>"},{"location":"power/#connecting-power-analyzer-to-the-computer","title":"Connecting power analyzer to the computer","text":"<p>We need to connect the power analyzer to a director machine via USB if the director machine is running Linux. Ethernet and serial modes are supported only on Windows. The power supply to the SUT is done through the power analyzer (current in series and voltage in parallel). An adapter like this can help avoid cutting the electrical wires. </p> <p>.</p> <p>The director machine runs the <code>server.py</code> script and loads a server process that communicates with the SPEC PTDaemon. When a client connects to it (using <code>client.py</code>), it in turn connects to the PTDaemon and initiates a measurement run. Once the measurement ends, the power log files are transferred to the client. </p>"},{"location":"power/#ranging-mode-and-testing-mode","title":"Ranging mode and Testing mode","text":"<p>Power analyzers usually have different current and voltage ranges it supports and the exact ranges to be used depends on a given SUT and this needs some empirical data. We can do a ranging run where the current and voltage ranges are set to <code>Auto</code> and the power analyzer automatically figures out the correct ranges needed. These determined ranges are then used for a proper testing mode run. Using the 'auto' mode in a testing run is not allowed as it can mess up the measurements.</p>"},{"location":"power/#start-power-server-power-analyzer-should-be-connected-to-this-computer-and-ptdaemon-runs-here","title":"Start Power Server (Power analyzer should be connected to this computer and PTDaemon runs here)","text":"<p>If you are having GitHub access to MLCommons power repository, PTDaemon should be automatically installed using the below MLC command:</p> <p>PS: The below command will ask for <code>sudo</code> permission on Linux and should be run with administrator privilege on Windows (to do NTP time sync). <pre><code>mlcr mlperf,power,server --device_type=49 --device_port=/dev/usbtmc0\n</code></pre> * <code>`--interface_flag=\"-U\" and</code>--device_port=1<code>(can change as per the USB slot used for connecting) can be used on Windows for USB connection *</code>--device_type=49<code>corresponds to Yokogawa 310E and</code>ptd -h<code>should list the device_type for all supported devices. The location of</code>ptd<code>can be found using the below command *</code>--device_port=20<code>and</code>--interface_flag=\"-g\" can be used to connect to GPIB interface (currently supported only on Windows) with the serial address set to 20 <pre><code>cat `mlc find cache --tags=get,spec,ptdaemon`/mlc-cached-state.json\n</code></pre></p> <p>An example analyzer configuration file <pre><code>[server]\nntpserver = time.google.com\nlisten = 0.0.0.0 4950\n\n[ptd]\nptd = C:\\Users\\arjun\\CM\\repos\\local\\cache\\5a0a52d578724774\\repo\\PTD\\binaries\\ptd-windows-x86.exe\nanalyzerCount = 2\n[analyzer2]\ninterfaceflag = -g\ndevicetype = 8\ndeviceport = 20\nnetworkport = 8888\n\n[analyzer1]\ninterfaceflag = -y\ndevicetype = 49\ndeviceport = C3YD21068E\nnetworkport = 8889\n</code></pre></p>"},{"location":"power/#running-the-power-server-inside-a-docker-container","title":"Running the power server inside a docker container","text":"<p><pre><code>mlc docker script --tags=run,mlperf,power,server --docker_gh_token=&lt;GITHUB AUTH_TOKEN&gt; \\\n--device=/dev/usbtmc0\n</code></pre> * Device address may need to be changed depending on the USB port being used * The above command uses a host-container port mapping 4950:4950 which can be changed by using <code>--docker_port_maps,=4950:4950</code></p>"},{"location":"power/#running-a-dummy-workload-with-power-on-host-machine","title":"Running a dummy workload with power (on host machine)","text":"<pre><code>mlcr mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt; \n</code></pre>"},{"location":"power/#run-a-dummy-workload-with-power-inside-a-docker-container","title":"Run a dummy workload with power inside a docker container","text":"<pre><code>mlc docker script --tags==mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt;\"\n</code></pre> <p>Note: The following commands are for demonstrating workflow runs with power measurements and do not execute any MLPerf Automotive reference implementation. They are based on the MLPerf Inference reference implementation.</p>"},{"location":"power/#running-mlperf-image-classification-with-power","title":"Running MLPerf Image Classification with power","text":"<pre><code>mlcr app,mlperf,inference,_reference,_power,_resnet50,_onnxruntime,_cpu --mode=performance --power_server=&lt;POWER_SERVER_IP&gt;\n</code></pre>"},{"location":"power/#running-mlperf-image-classification-with-power-inside-a-docker-container","title":"Running MLPerf Image Classification with power inside a docker container","text":"<pre><code>mlcr app,mlperf,inference,_reference,_power,_resnet50,_onnxruntime,_cpu --mode=performance --power_server=&lt;POWER_SERVER_IP&gt; --docker\n</code></pre>"},{"location":"submission/","title":"Submission Generation","text":"<p>Click here to view the proposal slide for Common Automation for MLPerf Inference Submission Generation through MLCFlow.</p> <p>Please refer to the installation page to install MLCFlow for automating the submission generation. In a typical development environment <code>pip install mlc-scripts</code> should be enough.</p> Custom automation based MLPerf resultsMLC automation based results <p>If you have not followed the <code>mlcr</code> commands under the individual model pages in the benchmarks directory, please make sure that the result directory is structured in the following way. You can see the real examples for the expected folder structure here. <pre><code>\u2514\u2500\u2500 System description ID(SUT Name)\n    \u251c\u2500\u2500 system_meta.json\n    \u2514\u2500\u2500 Benchmark\n        \u2514\u2500\u2500 Scenario\n            \u251c\u2500\u2500 Performance\n            |   \u2514\u2500\u2500 run_1 run for all scenarios\n            |       \u251c\u2500\u2500 mlperf_log_summary.txt\n            |       \u2514\u2500\u2500 mlperf_log_detail.txt\n            \u251c\u2500\u2500 Accuracy\n            |   \u251c\u2500\u2500 mlperf_log_summary.txt\n            |   \u251c\u2500\u2500 mlperf_log_detail.txt\n            |   \u251c\u2500\u2500 mlperf_log_accuracy.json\n            |   \u2514\u2500\u2500 accuracy.txt\n            |\u2500\u2500 Compliance_Test_ID\n            |   \u251c\u2500\u2500 Performance\n            |   |   \u2514\u2500\u2500 run_x/#1 run for all scenarios\n            |   |       \u251c\u2500\u2500 mlperf_log_summary.txt\n            |   |       \u2514\u2500\u2500 mlperf_log_detail.txt\n            |   \u251c\u2500\u2500 Accuracy # for TEST01 only\n            |   |   \u251c\u2500\u2500 baseline_accuracy.txt (if test fails in deterministic mode)\n            |   |   \u251c\u2500\u2500 compliance_accuracy.txt (if test fails in deterministic mode)\n            |   |   \u251c\u2500\u2500 mlperf_log_accuracy.json\n            |   |   \u2514\u2500\u2500 accuracy.txt\n            |   \u251c\u2500\u2500 verify_performance.txt\n            |   \u2514\u2500\u2500 verify_accuracy.txt # for TEST01 only\n            |\u2500\u2500 user.conf\n            \u2514\u2500\u2500 measurements.json\n</code></pre></p> <p> Click here if you are submitting in open division <ul> <li>The <code>model_mapping.json</code> should be included inside the SUT folder which is used to map the custom model full name to the official model name. The format of json file is:</li> </ul> <p><pre><code>    {\n        \"custom_model_name_for_model1\":\"official_model_name_for_model1\",\n        \"custom_model_name_for_model2\":\"official_model_name_for_model2\",\n\n    }\n</code></pre> </p> <p>If you have followed the <code>mlcr</code> commands under the individual model pages in the benchmarks directory, all the valid results will get aggregated to the <code>mlc cache</code> folder. The following command could be used to browse the structure of inference results folder generated by MLCFlow.</p> <p>Once all the results across all the models are ready you can use the following the below section to generate a valid submission tree compliant with the MLPerf requirements.</p>"},{"location":"submission/#get-results-folder-structure","title":"Get results folder structure","text":"<pre><code>mlc find cache --tags=get,mlperf,inference,results,dir | xargs tree\n</code></pre>"},{"location":"submission/#generate-submission-folder","title":"Generate submission folder","text":"<p>The submission generation flow is explained in the below diagram</p> <pre><code>flowchart LR\n    subgraph Generation [Submission Generation SUT1]\n      direction TB\n      A[populate system details] --&gt; B[generate submission structure]\n      B --&gt; C[truncate-accuracy-logs]\n      C --&gt; D{Infer low latency results &lt;br&gt;and/or&lt;br&gt; filter out invalid results}\n      D --&gt; yes --&gt; E[preprocess-mlperf-inference-submission]\n      D --&gt; no --&gt; F[run-mlperf-inference-submission-checker]\n      E --&gt; F\n    end\n    Input((Results SUT1)) --&gt; Generation\n    Generation --&gt; Output((Submission Folder &lt;br&gt; SUT1))</code></pre>"},{"location":"submission/#command-to-generate-submission-folder","title":"Command to generate submission folder","text":"<pre><code>mlcr generate,inference,submission,_wg-automotive \\\n  --clean \\\n  --preprocess_submission=yes \\\n  --run_checker=yes \\\n  --submitter=MLCommons \\\n  --division=closed \\\n  --env.MLC_DETERMINE_MEMORY_CONFIGURATION=yes \\\n  --quiet\n</code></pre> <p>Tip</p> <ul> <li> <p>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name. Examples can be seen here</p> </li> <li> <p>Use <code>--submitter=&lt;Your name&gt;</code> if your organization is an official MLCommons member and would like to submit under your organization</p> </li> <li> <p>Use <code>--hw_notes_extra</code> option to add additional notes like <code>--hw_notes_extra=\"Result taken by NAME\"</code></p> </li> <li> <p>Use <code>--results_dir</code> option to specify the results folder.  It is automatically taken from MLC cache for MLPerf automation based runs</p> </li> <li> <p>Use <code>--submission_dir</code> option to specify the submission folder. (You can avoid this if you're pushing to github or only running a single SUT and MLC will use its cache folder)</p> </li> <li> <p>Use <code>--division=open</code> for open division submission </p> </li> <li> <p>Use <code>--category</code> option to specify the category for which submission is generated(datacenter/edge). By default, the category is taken from <code>system_meta.json</code> file located in the SUT root directory.</p> </li> <li> <p>Use <code>--submission_base_dir</code> to specify the directory to which the outputs from preprocess submission script and final submission is added. No need to provide <code>--submission_dir</code> along with this. For <code>docker run</code>, use <code>--submission_base_dir</code> instead of <code>--submission_dir</code>.</p> </li> </ul> <p>If there are multiple systems where MLPerf results are collected, the same process needs to be repeated on each of them. One we have submission folders on all the SUTs, we need to sync them to make a single submission folder</p> Sync LocallySync via a Github repo <p>If you are having results in multiple systems, you need to merge them to one system. You can use <code>rsync</code> for this. For example, the below command will sync the submission folder from SUT2 to the one in SUT1.  <pre><code>rsync -avz username@host1:&lt;path_to_submission_folder2&gt;/ &lt;path_to_submission_folder1&gt;/\n</code></pre> Same needs to be repeated for all other SUTs so that we have the full submissions in SUT1.</p> <pre><code>    flowchart LR\n        subgraph SUT1 [Submission Generation SUT1]\n          A[Submission Folder SUT1]\n        end\n        subgraph SUT2 [Submission Generation SUT2]\n          B[Submission Folder SUT2]\n        end\n        subgraph SUT3 [Submission Generation SUT3]\n          C[Submission Folder SUT3]\n        end\n        subgraph SUTN [Submission Generation SUTN]\n          D[Submission Folder SUTN]\n        end\n        SUT2 --&gt; SUT1\n        SUT3 --&gt; SUT1\n        SUTN --&gt; SUT1\n</code></pre> <p>If you are collecting results across multiple systems you can generate different submissions and aggregate all of them to a GitHub repository (can be private) and use it to generate a single tar ball which can be uploaded to the MLCommons Submission UI. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub repository URL.</p> <pre><code>mlcr push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/mlcommons/mlperf_automotive_submissions_v0.5 \\\n   --commit_message=\"Results on &lt;HW name&gt; added by &lt;Name&gt;\" \\\n   --quiet\n</code></pre> <pre><code>    flowchart LR\n        subgraph SUT1 [Submission Generation SUT1]\n          A[Submission Folder SUT1]\n        end\n        subgraph SUT2 [Submission Generation SUT2]\n          B[Submission Folder SUT2]\n        end\n        subgraph SUT3 [Submission Generation SUT3]\n          C[Submission Folder SUT3]\n        end\n        subgraph SUTN [Submission Generation SUTN]\n          D[Submission Folder SUTN]\n        end\n    SUT2 -- git sync and push --&gt; G[Github Repo]\n    SUT3 -- git sync and push --&gt; G[Github Repo]\n    SUTN -- git sync and push --&gt; G[Github Repo]\n    SUT1 -- git sync and push --&gt; G[Github Repo]\n</code></pre>"},{"location":"submission/#upload-the-final-submission","title":"Upload the final submission","text":"<p>Warning</p> <p>If you are using GitHub for consolidating your results, make sure that you have run the <code>push-to-github</code> command on the same system to ensure results are synced as is on the GitHub repository.</p> <p>Once you have all the results on the system, you can upload them to the MLCommons submission server as follows:</p> via Browser <p>You can do the following command to generate the final submission tar file and then upload to the MLCommons Submission UI.  <pre><code>mlcr run,submission,checker,_wg-automotive \\\n--submission_dir=&lt;Path to the submission folder&gt; \\\n--tar=yes \\\n--submission_tar_file=mysubmission.tar.gz\n</code></pre></p> <pre><code>        flowchart LR\n            subgraph SUT [Combined Submissions]\n              A[Combined Submission Folder in SUT1]\n            end\n        SUT --&gt; B[Run submission checker]\n        B --&gt; C[Upload to MLC Submission server]\n        C --&gt; D[Receive validation email]</code></pre>"},{"location":"usage/","title":"Using MLC for MLPerf Inference","text":""}]}